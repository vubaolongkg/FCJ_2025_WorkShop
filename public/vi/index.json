[
{
	"uri": "//localhost:1313/vi/6-athena-analysis/6.1-configure-athena/",
	"title": "C·∫•u h√¨nh Athena",
	"tags": [],
	"description": "",
	"content": "6.1 C·∫•u h√¨nh Athena Truy c·∫≠p d·ªãch v·ª• Athena t·ª´ AWS Console ·ªû l·∫ßn s·ª≠ d·ª•ng ƒë·∫ßu ti√™n, c·∫•u h√¨nh Query result location: Ch·ªçn th∆∞ m·ª•c: s3://my-data-pipeline-bucket/query-results/ Nh·∫•n Save ƒë·ªÉ ho√†n t·∫•t c·∫•u h√¨nh "
},
{
	"uri": "//localhost:1313/vi/1-introduction/",
	"title": "Gi·ªõi thi·ªáu",
	"tags": [],
	"description": "",
	"content": "N·ªôi dung:\nüìå M·ª•c ti√™u d·ª± √°n üí° √ù nghƒ©a v√† ·ª©ng d·ª•ng üìå M·ª•c ti√™u d·ª± √°n D·ª± √°n h∆∞·ªõng ƒë·∫øn vi·ªác x√¢y d·ª±ng m·ªôt pipeline x·ª≠ l√Ω d·ªØ li·ªáu ho√†n ch·ªânh tr√™n n·ªÅn t·∫£ng AWS, s·ª≠ d·ª•ng c√°c d·ªãch v·ª• ch·ªß ch·ªët nh∆∞ Amazon S3, AWS Glue, v√† Amazon Athena. To√†n b·ªô quy tr√¨nh ETL s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông h√≥a nh·∫±m ƒë·∫£m b·∫£o hi·ªáu su·∫•t v√† ƒë·ªô ch√≠nh x√°c trong x·ª≠ l√Ω d·ªØ li·ªáu.\nüí° √ù nghƒ©a v√† ·ª©ng d·ª•ng H·ªó tr·ª£ doanh nghi·ªáp ho·∫∑c c√° nh√¢n qu·∫£n l√Ω, ph√¢n t√≠ch d·ªØ li·ªáu l·ªõn (Big Data) hi·ªáu qu·∫£ h∆°n. TƒÉng c∆∞·ªùng k·ªπ nƒÉng th·ª±c h√†nh v·ªõi c√°c c√¥ng c·ª• AWS th·ª±c t·∫ø. L√† n·ªÅn t·∫£ng ƒë·ªÉ m·ªü r·ªông ra c√°c gi·∫£i ph√°p BI nh∆∞ Amazon QuickSight cho tr·ª±c quan h√≥a d·ªØ li·ªáu. "
},
{
	"uri": "//localhost:1313/vi/8-quicksight-dashboard/8.1-connect-athena/",
	"title": "K·∫øt n·ªëi Athena v·ªõi QuickSight",
	"tags": [],
	"description": "",
	"content": "8.1 K·∫øt n·ªëi Athena v·ªõi QuickSight Truy c·∫≠p Amazon QuickSight ‚Üí Manage data Ch·ªçn New dataset ‚Üí Athena ƒê·∫∑t t√™n v√† ch·ªçn Athena workgroup Ch·ªçn database sales_db, table sales_processed Import d·ªØ li·ªáu ho·∫∑c d√πng ch·∫ø ƒë·ªô SPICE ƒë·ªÉ tƒÉng t·ªëc "
},
{
	"uri": "//localhost:1313/vi/5-crawler-processed/5.1-create-new-crawler/",
	"title": "T·∫°o Crawler m·ªõi",
	"tags": [],
	"description": "",
	"content": "5.1 T·∫°o Crawler m·ªõi Truy c·∫≠p AWS Glue ‚Üí Crawlers ‚Üí Add crawler Nh·∫≠p: T√™n: salesdatacrawler_processed Data source: s3://my-data-pipeline-bucket/processed/ IAM Role: s·ª≠ d·ª•ng role Glue ƒë√£ t·∫°o tr∆∞·ªõc ƒë√≥ Output: Database: sales_db T√™n b·∫£ng m·ªõi: sales_processed "
},
{
	"uri": "//localhost:1313/vi/3-crawler-raw/3.1-create-glue-database/",
	"title": "T·∫°o Glue Database",
	"tags": [],
	"description": "",
	"content": "3.1 T·∫°o Glue Database V√†o AWS Glue Console. Ch·ªçn Databases ‚Üí nh·∫•n Add database. ƒê·∫∑t t√™n v√≠ d·ª•: sales_db. Glue Database s·∫Ω l∆∞u c√°c b·∫£ng ƒë∆∞·ª£c sinh ra t·ª´ Glue Crawler.\n"
},
{
	"uri": "//localhost:1313/vi/4-glue-job/4.1-create-new-job/",
	"title": "T·∫°o Glue Job m·ªõi",
	"tags": [],
	"description": "",
	"content": "4.1 T·∫°o Glue Job m·ªõi V√†o AWS Glue ‚Üí ch·ªçn Jobs ‚Üí Add Job T√™n Job: ProcessJob IAM Role: ch·ªçn IAM ƒë√£ t·∫°o t·ª´ b∆∞·ªõc tr∆∞·ªõc Type: Spark, Python Script path: T·∫°o script x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ b·∫£ng sales_raw v√† ghi k·∫øt qu·∫£ ra s3://bucket/processed/ "
},
{
	"uri": "//localhost:1313/vi/2-prepare-environment/2.1-create-iam-role/",
	"title": "T·∫°o IAM Role",
	"tags": [],
	"description": "",
	"content": "T·∫°o IAM Role ƒê·ªÉ c√°c d·ªãch v·ª• AWS c√≥ th·ªÉ ho·∫°t ƒë·ªông ƒë√∫ng, c·∫ßn t·∫°o IAM Role v·ªõi quy·ªÅn th√≠ch h·ª£p:\nüîπ Cho Glue: AWSGlueServiceRole AmazonS3FullAccess üîπ Cho Lambda: AWSLambdaBasicExecutionRole AWSGlueConsoleFullAccess C√°c quy·ªÅn n√†y cho ph√©p Glue ƒë·ªçc/ghi d·ªØ li·ªáu t·ª´ S3 v√† Lambda c√≥ th·ªÉ g·ªçi Glue Job.\n"
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/7.1-create-lambda-function/",
	"title": "T·∫°o Lambda Function",
	"tags": [],
	"description": "",
	"content": "7.1 T·∫°o Lambda Function Truy c·∫≠p AWS Lambda ‚Üí Ch·ªçn Create function Ch·ªçn Author from scratch: T√™n: RunETLPipeline Runtime: Python 3.12 Role: ch·ªçn IAM Role ƒë√£ t·∫°o (c√≥ quy·ªÅn Glue + CloudWatch + S3) Trong ph·∫ßn function code, b·∫°n s·∫Ω vi·∫øt m√£ ƒë·ªÉ th·ª±c hi·ªán: Start crawler cho sales_raw Ch·∫°y Glue Job ProcessJob Ch·∫°y crawler cho sales_processed "
},
{
	"uri": "//localhost:1313/vi/",
	"title": "Workshop Pipeline AWS X·ª≠ l√Ω D·ªØ li·ªáu",
	"tags": [],
	"description": "",
	"content": "üëã Ch√†o m·ª´ng ƒë·∫øn v·ªõi Workshop AWS - FCJ 2025 Gi·ªõi thi·ªáu Trong workshop th·ª±c h√†nh n√†y, b·∫°n s·∫Ω h·ªçc c√°ch x√¢y d·ª±ng pipeline x·ª≠ l√Ω d·ªØ li·ªáu kh√¥ng d√πng m√°y ch·ªß (serverless) s·ª≠ d·ª•ng c√°c d·ªãch v·ª• AWS, t·ª´ vi·ªác nh·∫≠n d·ªØ li·ªáu CSV ƒë·∫ßu v√†o cho ƒë·∫øn hi·ªÉn th·ªã d·ªØ li·ªáu qua QuickSight. Ngo√†i ra b·∫°n c√≤n ƒë∆∞·ª£c t·ª± ƒë·ªông h√≥a quy tr√¨nh v·ªõi Lambda v√† b·∫£o m·∫≠t b·∫±ng IAM \u0026amp; CloudTrail.\nD·ªãch v·ª• S·ª≠ d·ª•ng Amazon S3 ‚Äì L∆∞u tr·ªØ d·ªØ li·ªáu ƒë·∫ßu v√†o v√† sau x·ª≠ l√Ω AWS Glue Crawler ‚Äì ƒê·ªçc v√† nh·∫≠n di·ªán c·∫•u tr√∫c CSV AWS Glue Job ‚Äì L√†m s·∫°ch v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu Amazon Athena ‚Äì Truy v·∫•n d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω Amazon QuickSight ‚Äì T·∫°o dashboard tr·ª±c quan AWS Lambda ‚Äì T·ª± ƒë·ªông h√≥a c√°c b∆∞·ªõc x·ª≠ l√Ω AWS CloudTrail ‚Äì Ghi l·∫°i ho·∫°t ƒë·ªông v√† gi√°m s√°t Quy tr√¨nh t·ªïng qu√°t M·ª•c ti√™u Workshop N·∫Øm ƒë∆∞·ª£c ki·∫øn tr√∫c x·ª≠ l√Ω d·ªØ li·ªáu serverless v·ªõi AWS L√†m s·∫°ch, t√≠nh to√°n d·ªØ li·ªáu v·ªõi Glue Truy v·∫•n k·∫øt qu·∫£ b·∫±ng Athena Hi·ªÉn th·ªã d·ªØ li·ªáu qua QuickSight T·ª± ƒë·ªông h√≥a x·ª≠ l√Ω v·ªõi Lambda Ghi nh·∫≠n v√† ki·ªÉm tra qua CloudTrail N·ªôi dung ch√≠nh Gi·ªõi thi·ªáu Pipeline v√† Ki·∫øn tr√∫c Chu·∫©n b·ªã m√¥i tr∆∞·ªùng AWS Crawler d·ªØ li·ªáu th√¥ X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Glue Job Crawler d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω Truy v·∫•n v·ªõi Athena T·ª± ƒë·ªông h√≥a b·∫±ng Lambda Dashboard v·ªõi QuickSight Xo√° v√† d·ªçn d·∫πp t√†i nguy√™n "
},
{
	"uri": "//localhost:1313/vi/2-prepare-environment/",
	"title": "C√°c b∆∞·ªõc chu·∫©n b·ªã",
	"tags": [],
	"description": "",
	"content": "T·ªïng quan c√°c b∆∞·ªõc chu·∫©n b·ªã Tr∆∞·ªõc khi tri·ªÉn khai pipeline x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Glue v√† S3, c·∫ßn chu·∫©n b·ªã c√°c t√†i nguy√™n v√† quy·ªÅn sau tr√™n AWS:\nN·ªôi dung: 2.1 T·∫°o IAM Role 2.2 T·∫°o Bucket S3 2.3 Upload d·ªØ li·ªáu m·∫´u üîê 1. IAM Role T·∫°o IAM Role cho Glue v√† Lambda ƒë·ªÉ ƒë·∫£m b·∫£o quy·ªÅn truy c·∫≠p d·ªãch v·ª• v√† S3.\nü™£ 2. S3 Bucket T·∫°o bucket ch·ª©a d·ªØ li·ªáu ƒë·∫ßu v√†o v√† ƒë·∫ßu ra, v·ªõi 2 th∆∞ m·ª•c ch√≠nh: raw/ v√† processed/.\nüìÇ 3. Upload d·ªØ li·ªáu m·∫´u Chu·∫©n b·ªã t·ªáp CSV ƒë∆°n gi·∫£n (v√≠ d·ª•: sales.csv) v√† upload v√†o th∆∞ m·ª•c raw/.\n"
},
{
	"uri": "//localhost:1313/vi/5-crawler-processed/5.2-run-and-verify/",
	"title": "Ch·∫°y Crawler v√† ki·ªÉm tra b·∫£ng",
	"tags": [],
	"description": "",
	"content": "5.2 Ch·∫°y Crawler v√† ki·ªÉm tra b·∫£ng Sau khi t·∫°o xong, ch·ªçn crawler salesdatacrawler_processed v√† nh·∫•n Run Crawler Truy c·∫≠p Glue ‚Üí Databases ‚Üí sales_db Ki·ªÉm tra b·∫£ng sales_processed ƒë√£ ƒë∆∞·ª£c t·∫°o ƒë√∫ng v·ªõi schema sau khi ETL Schema l√∫c n√†y ƒë√£ chu·∫©n h√≥a, ƒë√∫ng ƒë·ªãnh d·∫°ng theo y√™u c·∫ßu.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduction/1.1-project-goals/",
	"title": "M·ª•c ti√™u ƒë·ªÅ t√†i",
	"tags": [],
	"description": "",
	"content": "N·ªôi dung | Content:\nüéØ M·ª•c ti√™u t·ªïng quan ‚öôÔ∏è T·ª± ƒë·ªông h√≥a quy tr√¨nh ETL üìä H·ªó tr·ª£ truy v·∫•n v√† tr·ª±c quan h√≥a üéØ M·ª•c ti√™u t·ªïng quan X√¢y d·ª±ng pipeline x·ª≠ l√Ω d·ªØ li·ªáu s·ª≠ d·ª•ng c√°c d·ªãch v·ª• AWS: S3, Glue, v√† Athena. Thi·∫øt k·∫ø ki·∫øn tr√∫c linh ho·∫°t, c√≥ kh·∫£ nƒÉng m·ªü r·ªông, d·ªÖ d√†ng t√≠ch h·ª£p th√™m d·ªãch v·ª•. Build a data processing pipeline using AWS services: S3, Glue, and Athena.\nDesign a flexible, scalable architecture that supports easy integration.\n‚öôÔ∏è T·ª± ƒë·ªông h√≥a quy tr√¨nh ETL To√†n b·ªô quy tr√¨nh ETL (Extract, Transform, Load) s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông h√≥a b·∫±ng AWS Glue v√† c√°c t√°c v·ª• l·ªãch tr√¨nh. ƒê·∫£m b·∫£o x·ª≠ l√Ω d·ªØ li·ªáu ƒë·ªãnh k·ª≥, gi·∫£m thi·ªÉu l·ªói do thao t√°c th·ªß c√¥ng. The entire ETL process will be automated using AWS Glue and scheduled jobs.\nEnsure periodic data processing with minimal manual intervention.\nüìä H·ªó tr·ª£ truy v·∫•n v√† tr·ª±c quan h√≥a D·ªØ li·ªáu sau x·ª≠ l√Ω ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n S3 v√† truy v·∫•n b·∫±ng Athena th√¥ng qua SQL. T√πy ch·ªçn k·∫øt n·ªëi v·ªõi Amazon QuickSight ƒë·ªÉ tr·ª±c quan h√≥a d·ªØ li·ªáu qua bi·ªÉu ƒë·ªì, dashboard. Processed data is stored in S3 and queried using Athena with SQL.\nOptionally connect to Amazon QuickSight for visualization via charts and dashboards.\n"
},
{
	"uri": "//localhost:1313/vi/8-quicksight-dashboard/8.2-create-dashboard/",
	"title": "T·∫°o dashboard tr·ª±c quan",
	"tags": [],
	"description": "",
	"content": "8.2 T·∫°o dashboard tr·ª±c quan Ch·ªçn New analysis ‚Üí dataset sales_processed Th√™m bi·ªÉu ƒë·ªì: Bi·ªÉu ƒë·ªì c·ªôt: doanh thu theo ng√†y (order_date, amount) B·∫£ng d·ªØ li·ªáu: top s·∫£n ph·∫©m b√°n ch·∫°y (product, SUM(amount)) Tu·ª≥ ch·ªânh m√†u s·∫Øc, ti√™u ƒë·ªÅ v√† l·ªçc d·ªØ li·ªáu theo nhu c·∫ßu "
},
{
	"uri": "//localhost:1313/vi/3-crawler-raw/3.2-create-glue-crawler/",
	"title": "T·∫°o Glue Crawler",
	"tags": [],
	"description": "",
	"content": "3.2 T·∫°o Glue Crawler V√†o Crawlers ‚Üí ch·ªçn Add crawler Nh·∫≠p: T√™n: salesdatacrawler Data source: s3://my-data-pipeline-bucket/raw/ IAM Role: ch·ªçn role ƒë√£ t·∫°o Output: Database: sales_db B·∫£ng: sales_raw Crawler gi√∫p t·ª± ƒë·ªông ph√°t hi·ªán schema t·ª´ file CSV.\n"
},
{
	"uri": "//localhost:1313/vi/2-prepare-environment/2.2-create-s3-bucket/",
	"title": "T·∫°o S3 Bucket",
	"tags": [],
	"description": "",
	"content": "T·∫°o S3 Bucket Truy c·∫≠p d·ªãch v·ª• S3 t·ª´ AWS Console. Ch·ªçn Create Bucket, ƒë·∫∑t t√™n v√≠ d·ª•: my-data-pipeline-bucket T·∫°o 2 th∆∞ m·ª•c b√™n trong: raw/ ‚Äì ch·ª©a d·ªØ li·ªáu g·ªëc processed/ ‚Äì ch·ª©a d·ªØ li·ªáu sau x·ª≠ l√Ω "
},
{
	"uri": "//localhost:1313/vi/6-athena-analysis/6.2-basic-query/",
	"title": "Truy v·∫•n d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω",
	"tags": [],
	"description": "",
	"content": "6.2 Truy v·∫•n d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω Sau khi crawler ƒë√£ t·∫°o b·∫£ng sales_processed, b·∫°n c√≥ th·ªÉ truy v·∫•n th·ª≠:\nSELECT * FROM sales_db.sales_processed LIMIT 10; "
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/7.2-write-python-code/",
	"title": "Vi·∫øt m√£ Python cho Lambda",
	"tags": [],
	"description": "",
	"content": "7.2 Vi·∫øt m√£ Python cho Lambda S·ª≠ d·ª•ng th∆∞ vi·ªán boto3 ƒë·ªÉ g·ªçi AWS Glue APIs:\nimport boto3 import logging import time logger = logging.getLogger() logger.setLevel(logging.INFO) glue = boto3.client(\u0026#39;glue\u0026#39;) def lambda_handler(event, context): logger.info(\u0026#34;üîÅ Lambda triggered\u0026#34;) crawler_name = \u0026#39;salesdatacrawler\u0026#39; try: glue.start_crawler(Name=crawler_name) logger.info(f\u0026#34;‚úÖ Successfully started crawler: {crawler_name}\u0026#34;) except glue.exceptions.CrawlerRunningException: logger.warning(f\u0026#34;‚ö†Ô∏è Crawler {crawler_name} is already running.\u0026#34;) except Exception as e: logger.error(f\u0026#34;‚ùå Failed to start crawler {crawler_name}: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#34;Error starting crawler {crawler_name}: {str(e)}\u0026#34; } logger.info(f\u0026#34;‚è≥ Waiting for crawler {crawler_name} to complete...\u0026#34;) while True: crawler_state = glue.get_crawler(Name=crawler_name)[\u0026#39;Crawler\u0026#39;][\u0026#39;State\u0026#39;] if crawler_state == \u0026#39;READY\u0026#39;: logger.info(f\u0026#34;‚úÖ Crawler {crawler_name} finished.\u0026#34;) break time.sleep(10) job_name = \u0026#39;ProcessJob\u0026#39; try: while True: job_runs = glue.get_job_runs(JobName=job_name, MaxResults=1) if not job_runs[\u0026#39;JobRuns\u0026#39;]: break current_status = job_runs[\u0026#39;JobRuns\u0026#39;][0][\u0026#39;JobRunState\u0026#39;] if current_status in [\u0026#39;RUNNING\u0026#39;, \u0026#39;STARTING\u0026#39;, \u0026#39;STOPPING\u0026#39;]: logger.info(f\u0026#34;üïí Glue Job {job_name} is currently {current_status}, waiting...\u0026#34;) time.sleep(15) else: break logger.info(f\u0026#34;üöÄ Starting Glue Job: {job_name}\u0026#34;) response = glue.start_job_run(JobName=job_name) job_run_id = response[\u0026#39;JobRunId\u0026#39;] except Exception as e: logger.error(f\u0026#34;‚ùå Failed to start Glue Job: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#34;Error starting Glue job: {str(e)}\u0026#34; } logger.info(f\u0026#34;‚è≥ Waiting for Glue Job {job_name} to complete...\u0026#34;) while True: job_status = glue.get_job_run(JobName=job_name, RunId=job_run_id) state = job_status[\u0026#39;JobRun\u0026#39;][\u0026#39;JobRunState\u0026#39;] logger.info(f\u0026#34;üß™ Glue Job status: {state}\u0026#34;) if state in [\u0026#39;SUCCEEDED\u0026#39;, \u0026#39;FAILED\u0026#39;, \u0026#39;STOPPED\u0026#39;]: break time.sleep(15) if state != \u0026#39;SUCCEEDED\u0026#39;: logger.error(f\u0026#34;‚ùå Glue Job ended with status: {state}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#34;Glue Job failed or stopped with status: {state}\u0026#34; } logger.info(f\u0026#34;‚úÖ Glue Job {job_name} completed successfully.\u0026#34;) second_crawler = \u0026#39;salesdatacrawler_processed\u0026#39; try: glue.start_crawler(Name=second_crawler) logger.info(f\u0026#34;‚úÖ Successfully started crawler: {second_crawler}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#34;Workflow completed. Crawler {second_crawler} started.\u0026#34; } except Exception as e: logger.error(f\u0026#34;‚ùå Failed to start second crawler: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#34;Error starting second crawler {second_crawler}: {str(e)}\u0026#34; } ƒê·ª´ng qu√™n log tr·∫°ng th√°i ƒë·ªÉ ki·ªÉm tra trong CloudWatch Logs.\n"
},
{
	"uri": "//localhost:1313/vi/4-glue-job/4.2-write-etl-script/",
	"title": "Vi·∫øt script ETL",
	"tags": [],
	"description": "",
	"content": "4.2 Vi·∫øt script ETL Trong script Python (Spark):\nƒê·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng sales_raw ƒê·ªïi t√™n c·ªôt n·∫øu c·∫ßn (order_id ‚Üí id, v.v.) √âp ki·ªÉu d·ªØ li·ªáu ph√π h·ª£p (v√≠ d·ª•: date, float) Ghi ra s3://my-data-pipeline-bucket/processed/ d∆∞·ªõi d·∫°ng Parquet ho·∫∑c CSV import sys from pyspark.context import SparkContext from awsglue.utils import getResolvedOptions from awsglue.context import GlueContext from awsglue.job import Job from pyspark.sql.functions import col, trim, regexp_replace, round from awsglue.dynamicframe import DynamicFrame args = getResolvedOptions(sys.argv, [\u0026#39;JOB_NAME\u0026#39;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[\u0026#39;JOB_NAME\u0026#39;], args) input_dyf = glueContext.create_dynamic_frame.from_catalog( database=\u0026#34;sales_analysis_db\u0026#34;, table_name=\u0026#34;raw_sales_dataset\u0026#34; ) df = input_dyf.toDF() for col_name in df.columns: if df.schema[col_name].dataType.simpleString() == \u0026#39;string\u0026#39;: df = df.withColumn( col_name, trim( regexp_replace( regexp_replace(col(col_name), \u0026#39;^\u0026#34;|\u0026#34;$\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;\\s+\u0026#39;, \u0026#39; \u0026#39; ) ) ) if \u0026#39;price\u0026#39; in df.columns and \u0026#39;quantity\u0026#39; in df.columns: df = df.withColumn(\u0026#34;revenue\u0026#34;, round(col(\u0026#34;price\u0026#34;) * col(\u0026#34;quantity\u0026#34;), 2)) output_dyf = DynamicFrame.fromDF(df, glueContext, \u0026#34;output_dyf\u0026#34;) output_path = \u0026#34;s3://your-data-pipeline-bucket/processed/\u0026#34; glueContext.write_dynamic_frame.from_options( frame=output_dyf, connection_type=\u0026#34;s3\u0026#34;, connection_options={ \u0026#34;path\u0026#34;: output_path, \u0026#34;partitionKeys\u0026#34;: [] }, format=\u0026#34;csv\u0026#34; ) glueContext.write_dynamic_frame.from_options( frame=output_dyf, connection_type=\u0026#34;s3\u0026#34;, connection_options={\u0026#34;path\u0026#34;: \u0026#34;s3://your-data-pipeline-bucket/processed/\u0026#34;}, format=\u0026#34;csv\u0026#34; ) job.commit() N√™n d√πng ƒë·ªãnh d·∫°ng Parquet ƒë·ªÉ t·ªëi ∆∞u chi ph√≠ \u0026amp; hi·ªáu nƒÉng khi query b·∫±ng Athena.\n"
},
{
	"uri": "//localhost:1313/vi/3-crawler-raw/3.3-run-crawler/",
	"title": "Ch·∫°y Crawler",
	"tags": [],
	"description": "",
	"content": "3.3 Ch·∫°y Glue Crawler Sau khi t·∫°o xong, ch·ªçn crawler salesdatacrawler v√† nh·∫•n Run Crawler. Ki·ªÉm tra b·∫£ng: V√†o Glue Console ‚Üí Databases ‚Üí sales_db ƒê·∫£m b·∫£o b·∫£ng sales_raw ƒë√∫ng v·ªõi d·ªØ li·ªáu t·ª´ sales.csv. ƒê√¢y l√† b∆∞·ªõc x√°c minh schema ƒë∆∞·ª£c tr√≠ch xu·∫•t ch√≠nh x√°c.\n"
},
{
	"uri": "//localhost:1313/vi/4-glue-job/4.3-run-and-verify/",
	"title": "Ch·∫°y Job &amp; ki·ªÉm tra output",
	"tags": [],
	"description": "",
	"content": "4.3 Ch·∫°y Job \u0026amp; ki·ªÉm tra output V√†o Glue ‚Üí Jobs ‚Üí ch·ªçn ProcessJob ‚Üí Run Theo d√µi tr·∫°ng th√°i job: RUNNING ‚Üí SUCCEEDED M·ªü S3 Bucket ‚Üí th∆∞ m·ª•c processed/ Ki·ªÉm tra file ƒë∆∞·ª£c sinh ra c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng: Parquet ho·∫∑c CSV "
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/7.3-attach-s3-trigger/",
	"title": "G·∫Øn trigger S3 event",
	"tags": [],
	"description": "",
	"content": "7.3 G·∫Øn trigger S3 event Truy c·∫≠p S3 ‚Üí ch·ªçn bucket ch·ª©a d·ªØ li·ªáu (my-data-pipeline-bucket) V√†o tab Properties ‚Üí Event notifications Th√™m trigger: Prefix: raw/ Event type: PUT (ObjectCreated) Destination: Lambda function RunETLPipeline M·ªói khi c√≥ file m·ªõi trong th∆∞ m·ª•c raw/, Lambda s·∫Ω t·ª± ƒë·ªông ch·∫°y pipeline.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduction/1.2-purpose-and-applications/",
	"title": "M·ª•c ƒë√≠ch v√† ·ª®ng d·ª•ng",
	"tags": [],
	"description": "",
	"content": "N·ªôi dung:\nüåê T·∫°i sao d√πng AWS cho Data Pipeline? üìå Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng th·ª±c t·∫ø üöÄ Kh·∫£ nƒÉng m·ªü r·ªông trong t∆∞∆°ng lai üåê T·∫°i sao d√πng AWS cho Data Pipeline? AWS cung c·∫•p c√°c d·ªãch v·ª• m·∫°nh m·∫Ω nh∆∞ S3, Glue, v√† Athena, gi√∫p x√¢y d·ª±ng h·ªá th·ªëng x·ª≠ l√Ω d·ªØ li·ªáu hi·ªáu qu·∫£, kh√¥ng c·∫ßn qu·∫£n l√Ω h·∫° t·∫ßng ph·ª©c t·∫°p, v√† h·ªó tr·ª£ t·ª± ƒë·ªông h√≥a to√†n di·ªán.\nV·ªõi m√¥ h√¨nh t√≠nh ph√≠ theo m·ª©c s·ª≠ d·ª•ng, kh·∫£ nƒÉng m·ªü r·ªông to√†n c·∫ßu v√† t√≠ch h·ª£p d·ªãch v·ª• linh ho·∫°t, AWS l√† l·ª±a ch·ªçn h√†ng ƒë·∫ßu cho c√°c b√†i to√°n k·ªπ thu·∫≠t d·ªØ li·ªáu hi·ªán ƒë·∫°i.\nüìå Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng th·ª±c t·∫ø üîç L√†m s·∫°ch v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu: T·ª± ƒë·ªông x·ª≠ l√Ω, chu·∫©n h√≥a file CSV, JSON ho·∫∑c log v·ªõi Glue üìä Ph√¢n t√≠ch kinh doanh: Truy v·∫•n tr·ª±c ti·∫øp d·ªØ li·ªáu tr√™n S3 b·∫±ng SQL qua Athena üè¢ Chu·∫©n b·ªã d·ªØ li·ªáu cho kho d·ªØ li·ªáu: Chuy·ªÉn d·ªØ li·ªáu v·ªÅ d·∫°ng c√≥ c·∫•u tr√∫c ƒë·ªÉ ƒë∆∞a v√†o Redshift ho·∫∑c c√¥ng c·ª• BI kh√°c üîÅ C·∫≠p nh·∫≠t d·ªØ li·ªáu ƒë·ªãnh k·ª≥: Ch·∫°y c√°c job theo l·ªãch ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu b√°o c√°o üöÄ Kh·∫£ nƒÉng m·ªü r·ªông trong t∆∞∆°ng lai D·ªÖ d√†ng t√≠ch h·ª£p th√™m c√°c d·ªãch v·ª• nh∆∞:\nAmazon QuickSight ƒë·ªÉ tr·ª±c quan h√≥a Amazon Redshift l√†m kho d·ªØ li·ªáu AWS Lambda ƒë·ªÉ k√≠ch ho·∫°t t·ª± ƒë·ªông theo s·ª± ki·ªán H·ªó tr·ª£ th√™m ngu·ªìn d·ªØ li·ªáu v√† dung l∆∞·ª£ng l·ªõn m√† kh√¥ng c·∫ßn thay ƒë·ªïi h·ªá th·ªëng hi·ªán t·∫°i.\nKi·∫øn tr√∫c c√≥ t√≠nh m√¥-ƒëun, linh ho·∫°t v√† ‚Äúcloud-native‚Äù, ph√π h·ª£p cho m·ªçi nhu c·∫ßu m·ªü r·ªông v·ªÅ sau.\n"
},
{
	"uri": "//localhost:1313/vi/3-crawler-raw/",
	"title": "T·∫°o Glue Crawler",
	"tags": [],
	"description": "",
	"content": "T·∫°o Glue Crawler ƒê·ªÉ AWS Glue hi·ªÉu ƒë∆∞·ª£c c·∫•u tr√∫c d·ªØ li·ªáu trong S3, b·∫°n c·∫ßn th·ª±c hi·ªán 3 b∆∞·ªõc:\n3.1 T·∫°o Glue Database 3.2 T·∫°o Glue Crawler 3.3 Ch·∫°y Glue Crawler "
},
{
	"uri": "//localhost:1313/vi/2-prepare-environment/2.3-upload-sample-data/",
	"title": "Upload d·ªØ li·ªáu m·∫´u",
	"tags": [],
	"description": "",
	"content": "Upload d·ªØ li·ªáu m·∫´u Chu·∫©n b·ªã t·ªáp m·∫´u: sales.csv, ƒë·ªãnh d·∫°ng CSV ƒë∆°n gi·∫£n (v√≠ d·ª•: id, date, amount) Truy c·∫≠p S3 Bucket ƒë√£ t·∫°o Upload t·ªáp sales.csv v√†o th∆∞ m·ª•c raw/ "
},
{
	"uri": "//localhost:1313/vi/6-athena-analysis/6.3-advanced-queries/",
	"title": "Vi·∫øt truy v·∫•n n√¢ng cao",
	"tags": [],
	"description": "",
	"content": "6.3 Vi·∫øt c√°c c√¢u truy v·∫•n n√¢ng cao (tu·ª≥ ch·ªçn) M·ªôt s·ªë v√≠ d·ª• truy v·∫•n ph·ªï bi·∫øn:\nT·ªïng doanh thu theo s·∫£n ph·∫©m: SELECT product, SUM(amount) AS total_amount FROM sales_db.sales_processed GROUP BY product; Doanh thu theo ng√†y: SELECT order_date, SUM(amount) AS total FROM sales_db.sales_processed GROUP BY order_date ORDER BY order_date; "
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/7.4-cloudtrail-monitoring/",
	"title": "Ghi l·∫°i h√†nh vi ng∆∞·ªùi d√πng v·ªõi AWS CloudTrail",
	"tags": [],
	"description": "",
	"content": "M·ª•c ƒë√≠ch AWS CloudTrail gi√∫p theo d√µi v√† ghi nh·∫≠t k√Ω t·∫•t c·∫£ ho·∫°t ƒë·ªông c·ªßa ng∆∞·ªùi d√πng v√† d·ªãch v·ª• AWS, bao g·ªìm c√°c thao t√°c tr√™n S3, IAM, Lambda, v√† nhi·ªÅu d·ªãch v·ª• kh√°c.\nC√°c b∆∞·ªõc th·ª±c hi·ªán 1. Truy c·∫≠p d·ªãch v·ª• CloudTrail V√†o AWS Console ‚Üí T√¨m CloudTrail ‚Üí Nh·∫•n Create trail 2. T·∫°o Trail m·ªõi Trail name: fcj-trail Storage location: T·∫°o ho·∫∑c ch·ªçn 1 S3 Bucket ƒë·ªÉ ch·ª©a log B·∫≠t t√πy ch·ªçn Log file validation B·∫≠t ch·∫ø ƒë·ªô: Multi-region trail 3. C·∫•u h√¨nh nh·∫≠t k√Ω Ch·ªçn ghi nh·∫≠t k√Ω Management events Lo·∫°i h√†nh ƒë·ªông: Write-only B·∫≠t t√πy ch·ªçn: Data events ‚Üí Ch·ªçn S3 ‚Üí Add bucket: Bucket name: your-data-pipeline-bucket Event type: PUT (ƒë·ªÉ ghi l·∫°i h√†nh vi ghi d·ªØ li·ªáu) 4. Ki·ªÉm tra ho·∫°t ƒë·ªông Upload m·ªôt file CSV m·ªõi v√†o S3 Truy c·∫≠p l·∫°i CloudTrail ‚Üí Event history L·ªçc theo EventName = PutObject v√† Resource = your-data-pipeline-bucket B·∫°n s·∫Ω th·∫•y ai (IAM user ho·∫∑c Lambda) ƒë√£ thao t√°c, v√†o l√∫c n√†o L·ª£i √≠ch Gi√∫p ki·ªÉm tra xem ai ƒë√£ upload file n√†o, l√∫c n√†o Ph√°t hi·ªán b·∫•t th∆∞·ªùng trong thao t√°c d·ªØ li·ªáu Ph·ª•c v·ª• m·ª•c ƒë√≠ch audit v√† b·∫£o m·∫≠t G·ª£i √Ω B·∫°n c√≥ th·ªÉ k·∫øt h·ª£p CloudTrail v·ªõi:\nAWS CloudWatch Logs ƒë·ªÉ gi√°m s√°t theo th·ªùi gian th·ª±c AWS Config ƒë·ªÉ theo d√µi c·∫•u h√¨nh t√†i nguy√™n "
},
{
	"uri": "//localhost:1313/vi/4-glue-job/",
	"title": "T·∫°o Glue Job",
	"tags": [],
	"description": "",
	"content": "T·∫°o Glue Job ƒê·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ sales_raw v√† ghi k·∫øt qu·∫£ ra th∆∞ m·ª•c processed/, b·∫°n c·∫ßn:\n4.1 T·∫°o Glue Job m·ªõi 4.2 Vi·∫øt script ETL 4.3 Ch·∫°y Job \u0026amp; ki·ªÉm tra k·∫øt qu·∫£ "
},
{
	"uri": "//localhost:1313/vi/5-crawler-processed/",
	"title": "T·∫°o Glue Crawler cho d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω",
	"tags": [],
	"description": "",
	"content": "T·∫°o Glue Crawler cho d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω Sau khi d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† ghi v√†o th∆∞ m·ª•c processed/, ch√∫ng ta c·∫ßn t·∫°o m·ªôt Glue Crawler m·ªõi ƒë·ªÉ ph√°t hi·ªán schema c·ªßa d·ªØ li·ªáu ƒë·∫ßu ra.\nC√°c b∆∞·ªõc: 5.1 T·∫°o Crawler m·ªõi 5.2 Ch·∫°y Crawler v√† ki·ªÉm tra b·∫£ng "
},
{
	"uri": "//localhost:1313/vi/6-athena-analysis/",
	"title": "Truy v·∫•n v·ªõi Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Truy v·∫•n v·ªõi Amazon Athena Sau khi crawler ƒë√£ t·∫°o b·∫£ng sales_processed, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng Amazon Athena ƒë·ªÉ truy v·∫•n d·ªØ li·ªáu tr·ª±c ti·∫øp tr√™n S3.\nC√°c b∆∞·ªõc: 6.1 C·∫•u h√¨nh Athena 6.2 Truy v·∫•n d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω 6.3 Vi·∫øt c√°c c√¢u truy v·∫•n n√¢ng cao (tu·ª≥ ch·ªçn) "
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/",
	"title": "T·ª± ƒë·ªông h√≥a b·∫±ng AWS Lambda",
	"tags": [],
	"description": "",
	"content": "T·ª± ƒë·ªông h√≥a b·∫±ng AWS Lambda ƒê·ªÉ t·ª± ƒë·ªông h√≥a to√†n b·ªô pipeline, ch√∫ng ta s·ª≠ d·ª•ng AWS Lambda ƒë·ªÉ k√≠ch ho·∫°t qu√° tr√¨nh ETL khi c√≥ file m·ªõi.\nC√°c b∆∞·ªõc: 7.1 T·∫°o Lambda Function 7.2 Vi·∫øt m√£ Python cho Lambda 7.3 G·∫Øn trigger S3 event "
},
{
	"uri": "//localhost:1313/vi/8-quicksight-dashboard/",
	"title": "Tr·ª±c quan h√≥a v·ªõi Amazon QuickSight",
	"tags": [],
	"description": "",
	"content": "Tr·ª±c quan h√≥a v·ªõi Amazon QuickSight (Tu·ª≥ ch·ªçn) Sau khi d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† l∆∞u trong sales_processed, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng Amazon QuickSight ƒë·ªÉ t·∫°o c√°c dashboard tr·ª±c quan.\nC√°c b∆∞·ªõc: 8.1 K·∫øt n·ªëi Athena v·ªõi QuickSight 8.2 T·∫°o dashboard tr·ª±c quan "
},
{
	"uri": "//localhost:1313/vi/9-cleanup/",
	"title": "D·ªçn d·∫πp t√†i nguy√™n",
	"tags": [],
	"description": "",
	"content": "D·ªçn d·∫πp t√†i nguy√™n Sau khi ho√†n t·∫•t pipeline, b·∫°n n√™n d·ªçn d·∫πp c√°c t√†i nguy√™n AWS ƒë·ªÉ tr√°nh ph√°t sinh chi ph√≠ kh√¥ng c·∫ßn thi·∫øt.\nüßπ Xo√° t√†i nguy√™n theo danh m·ª•c: AWS Glue Xo√° Crawler: salesdatacrawler, salesdatacrawler_processed Xo√° Job: ProcessJob Xo√° Database: sales_db AWS Lambda Xo√° function: RunETLPipeline Amazon S3 Xo√° bucket: my-data-pipeline-bucket (ho·∫∑c ch·ªâ xo√° th∆∞ m·ª•c raw/, processed/ n·∫øu gi·ªØ bucket) IAM Xo√° c√°c IAM Role n·∫øu kh√¥ng t√°i s·ª≠ d·ª•ng: AWSGlueServiceRole AWSLambdaExecutionRole CloudWatch V√†o CloudWatch ‚Üí Logs ‚Üí Xo√° log group c·ªßa Lambda QuickSight V√†o QuickSight: Xo√° dataset sales_processed Xo√° dashboard n·∫øu ƒë√£ t·∫°o "
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]