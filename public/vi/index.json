[
{
	"uri": "//localhost:1313/vi/6-athena-analysis/6.1-configure-athena/",
	"title": "Cấu hình Athena",
	"tags": [],
	"description": "",
	"content": "6.1 Cấu hình Athena Truy cập dịch vụ Athena từ AWS Console Ở lần sử dụng đầu tiên, cấu hình Query result location: Chọn thư mục: s3://my-data-pipeline-bucket/query-results/ Nhấn Save để hoàn tất cấu hình "
},
{
	"uri": "//localhost:1313/vi/1-introduction/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Nội dung:\n📌 Mục tiêu dự án 💡 Ý nghĩa và ứng dụng 📌 Mục tiêu dự án Dự án hướng đến việc xây dựng một pipeline xử lý dữ liệu hoàn chỉnh trên nền tảng AWS, sử dụng các dịch vụ chủ chốt như Amazon S3, AWS Glue, và Amazon Athena. Toàn bộ quy trình ETL sẽ được tự động hóa nhằm đảm bảo hiệu suất và độ chính xác trong xử lý dữ liệu.\n💡 Ý nghĩa và ứng dụng Hỗ trợ doanh nghiệp hoặc cá nhân quản lý, phân tích dữ liệu lớn (Big Data) hiệu quả hơn. Tăng cường kỹ năng thực hành với các công cụ AWS thực tế. Là nền tảng để mở rộng ra các giải pháp BI như Amazon QuickSight cho trực quan hóa dữ liệu. "
},
{
	"uri": "//localhost:1313/vi/8-quicksight-dashboard/8.1-connect-athena/",
	"title": "Kết nối Athena với QuickSight",
	"tags": [],
	"description": "",
	"content": "8.1 Kết nối Athena với QuickSight Truy cập Amazon QuickSight → Manage data Chọn New dataset → Athena Đặt tên và chọn Athena workgroup Chọn database sales_db, table sales_processed Import dữ liệu hoặc dùng chế độ SPICE để tăng tốc "
},
{
	"uri": "//localhost:1313/vi/5-crawler-processed/5.1-create-new-crawler/",
	"title": "Tạo Crawler mới",
	"tags": [],
	"description": "",
	"content": "5.1 Tạo Crawler mới Truy cập AWS Glue → Crawlers → Add crawler Nhập: Tên: salesdatacrawler_processed Data source: s3://my-data-pipeline-bucket/processed/ IAM Role: sử dụng role Glue đã tạo trước đó Output: Database: sales_db Tên bảng mới: sales_processed "
},
{
	"uri": "//localhost:1313/vi/3-crawler-raw/3.1-create-glue-database/",
	"title": "Tạo Glue Database",
	"tags": [],
	"description": "",
	"content": "3.1 Tạo Glue Database Vào AWS Glue Console. Chọn Databases → nhấn Add database. Đặt tên ví dụ: sales_db. Glue Database sẽ lưu các bảng được sinh ra từ Glue Crawler.\n"
},
{
	"uri": "//localhost:1313/vi/4-glue-job/4.1-create-new-job/",
	"title": "Tạo Glue Job mới",
	"tags": [],
	"description": "",
	"content": "4.1 Tạo Glue Job mới Vào AWS Glue → chọn Jobs → Add Job Tên Job: ProcessJob IAM Role: chọn IAM đã tạo từ bước trước Type: Spark, Python Script path: Tạo script xử lý dữ liệu từ bảng sales_raw và ghi kết quả ra s3://bucket/processed/ "
},
{
	"uri": "//localhost:1313/vi/2-prepare-environment/2.1-create-iam-role/",
	"title": "Tạo IAM Role",
	"tags": [],
	"description": "",
	"content": "Tạo IAM Role Để các dịch vụ AWS có thể hoạt động đúng, cần tạo IAM Role với quyền thích hợp:\n🔹 Cho Glue: AWSGlueServiceRole AmazonS3FullAccess 🔹 Cho Lambda: AWSLambdaBasicExecutionRole AWSGlueConsoleFullAccess Các quyền này cho phép Glue đọc/ghi dữ liệu từ S3 và Lambda có thể gọi Glue Job.\n"
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/7.1-create-lambda-function/",
	"title": "Tạo Lambda Function",
	"tags": [],
	"description": "",
	"content": "7.1 Tạo Lambda Function Truy cập AWS Lambda → Chọn Create function Chọn Author from scratch: Tên: RunETLPipeline Runtime: Python 3.12 Role: chọn IAM Role đã tạo (có quyền Glue + CloudWatch + S3) Trong phần function code, bạn sẽ viết mã để thực hiện: Start crawler cho sales_raw Chạy Glue Job ProcessJob Chạy crawler cho sales_processed "
},
{
	"uri": "//localhost:1313/vi/",
	"title": "Workshop Pipeline AWS Xử lý Dữ liệu",
	"tags": [],
	"description": "",
	"content": "👋 Chào mừng đến với Workshop AWS - FCJ 2025 Giới thiệu Trong workshop thực hành này, bạn sẽ học cách xây dựng pipeline xử lý dữ liệu không dùng máy chủ (serverless) sử dụng các dịch vụ AWS, từ việc nhận dữ liệu CSV đầu vào cho đến hiển thị dữ liệu qua QuickSight. Ngoài ra bạn còn được tự động hóa quy trình với Lambda và bảo mật bằng IAM \u0026amp; CloudTrail.\nDịch vụ Sử dụng Amazon S3 – Lưu trữ dữ liệu đầu vào và sau xử lý AWS Glue Crawler – Đọc và nhận diện cấu trúc CSV AWS Glue Job – Làm sạch và chuyển đổi dữ liệu Amazon Athena – Truy vấn dữ liệu đã xử lý Amazon QuickSight – Tạo dashboard trực quan AWS Lambda – Tự động hóa các bước xử lý AWS CloudTrail – Ghi lại hoạt động và giám sát Quy trình tổng quát Mục tiêu Workshop Nắm được kiến trúc xử lý dữ liệu serverless với AWS Làm sạch, tính toán dữ liệu với Glue Truy vấn kết quả bằng Athena Hiển thị dữ liệu qua QuickSight Tự động hóa xử lý với Lambda Ghi nhận và kiểm tra qua CloudTrail Nội dung chính Giới thiệu Pipeline và Kiến trúc Chuẩn bị môi trường AWS Crawler dữ liệu thô Xử lý dữ liệu với Glue Job Crawler dữ liệu đã xử lý Truy vấn với Athena Tự động hóa bằng Lambda Dashboard với QuickSight Xoá và dọn dẹp tài nguyên "
},
{
	"uri": "//localhost:1313/vi/2-prepare-environment/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Tổng quan các bước chuẩn bị Trước khi triển khai pipeline xử lý dữ liệu với Glue và S3, cần chuẩn bị các tài nguyên và quyền sau trên AWS:\nNội dung: 2.1 Tạo IAM Role 2.2 Tạo Bucket S3 2.3 Upload dữ liệu mẫu 🔐 1. IAM Role Tạo IAM Role cho Glue và Lambda để đảm bảo quyền truy cập dịch vụ và S3.\n🪣 2. S3 Bucket Tạo bucket chứa dữ liệu đầu vào và đầu ra, với 2 thư mục chính: raw/ và processed/.\n📂 3. Upload dữ liệu mẫu Chuẩn bị tệp CSV đơn giản (ví dụ: sales.csv) và upload vào thư mục raw/.\n"
},
{
	"uri": "//localhost:1313/vi/5-crawler-processed/5.2-run-and-verify/",
	"title": "Chạy Crawler và kiểm tra bảng",
	"tags": [],
	"description": "",
	"content": "5.2 Chạy Crawler và kiểm tra bảng Sau khi tạo xong, chọn crawler salesdatacrawler_processed và nhấn Run Crawler Truy cập Glue → Databases → sales_db Kiểm tra bảng sales_processed đã được tạo đúng với schema sau khi ETL Schema lúc này đã chuẩn hóa, đúng định dạng theo yêu cầu.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduction/1.1-project-goals/",
	"title": "Mục tiêu đề tài",
	"tags": [],
	"description": "",
	"content": "Nội dung | Content:\n🎯 Mục tiêu tổng quan ⚙️ Tự động hóa quy trình ETL 📊 Hỗ trợ truy vấn và trực quan hóa 🎯 Mục tiêu tổng quan Xây dựng pipeline xử lý dữ liệu sử dụng các dịch vụ AWS: S3, Glue, và Athena. Thiết kế kiến trúc linh hoạt, có khả năng mở rộng, dễ dàng tích hợp thêm dịch vụ. Build a data processing pipeline using AWS services: S3, Glue, and Athena.\nDesign a flexible, scalable architecture that supports easy integration.\n⚙️ Tự động hóa quy trình ETL Toàn bộ quy trình ETL (Extract, Transform, Load) sẽ được tự động hóa bằng AWS Glue và các tác vụ lịch trình. Đảm bảo xử lý dữ liệu định kỳ, giảm thiểu lỗi do thao tác thủ công. The entire ETL process will be automated using AWS Glue and scheduled jobs.\nEnsure periodic data processing with minimal manual intervention.\n📊 Hỗ trợ truy vấn và trực quan hóa Dữ liệu sau xử lý được lưu trữ trên S3 và truy vấn bằng Athena thông qua SQL. Tùy chọn kết nối với Amazon QuickSight để trực quan hóa dữ liệu qua biểu đồ, dashboard. Processed data is stored in S3 and queried using Athena with SQL.\nOptionally connect to Amazon QuickSight for visualization via charts and dashboards.\n"
},
{
	"uri": "//localhost:1313/vi/8-quicksight-dashboard/8.2-create-dashboard/",
	"title": "Tạo dashboard trực quan",
	"tags": [],
	"description": "",
	"content": "8.2 Tạo dashboard trực quan Chọn New analysis → dataset sales_processed Thêm biểu đồ: Biểu đồ cột: doanh thu theo ngày (order_date, amount) Bảng dữ liệu: top sản phẩm bán chạy (product, SUM(amount)) Tuỳ chỉnh màu sắc, tiêu đề và lọc dữ liệu theo nhu cầu "
},
{
	"uri": "//localhost:1313/vi/3-crawler-raw/3.2-create-glue-crawler/",
	"title": "Tạo Glue Crawler",
	"tags": [],
	"description": "",
	"content": "3.2 Tạo Glue Crawler Vào Crawlers → chọn Add crawler Nhập: Tên: salesdatacrawler Data source: s3://my-data-pipeline-bucket/raw/ IAM Role: chọn role đã tạo Output: Database: sales_db Bảng: sales_raw Crawler giúp tự động phát hiện schema từ file CSV.\n"
},
{
	"uri": "//localhost:1313/vi/2-prepare-environment/2.2-create-s3-bucket/",
	"title": "Tạo S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Tạo S3 Bucket Truy cập dịch vụ S3 từ AWS Console. Chọn Create Bucket, đặt tên ví dụ: my-data-pipeline-bucket Tạo 2 thư mục bên trong: raw/ – chứa dữ liệu gốc processed/ – chứa dữ liệu sau xử lý "
},
{
	"uri": "//localhost:1313/vi/6-athena-analysis/6.2-basic-query/",
	"title": "Truy vấn dữ liệu đã xử lý",
	"tags": [],
	"description": "",
	"content": "6.2 Truy vấn dữ liệu đã xử lý Sau khi crawler đã tạo bảng sales_processed, bạn có thể truy vấn thử:\nSELECT * FROM sales_db.sales_processed LIMIT 10; "
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/7.2-write-python-code/",
	"title": "Viết mã Python cho Lambda",
	"tags": [],
	"description": "",
	"content": "7.2 Viết mã Python cho Lambda Sử dụng thư viện boto3 để gọi AWS Glue APIs:\nimport boto3 import logging import time logger = logging.getLogger() logger.setLevel(logging.INFO) glue = boto3.client(\u0026#39;glue\u0026#39;) def lambda_handler(event, context): logger.info(\u0026#34;🔁 Lambda triggered\u0026#34;) crawler_name = \u0026#39;salesdatacrawler\u0026#39; try: glue.start_crawler(Name=crawler_name) logger.info(f\u0026#34;✅ Successfully started crawler: {crawler_name}\u0026#34;) except glue.exceptions.CrawlerRunningException: logger.warning(f\u0026#34;⚠️ Crawler {crawler_name} is already running.\u0026#34;) except Exception as e: logger.error(f\u0026#34;❌ Failed to start crawler {crawler_name}: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#34;Error starting crawler {crawler_name}: {str(e)}\u0026#34; } logger.info(f\u0026#34;⏳ Waiting for crawler {crawler_name} to complete...\u0026#34;) while True: crawler_state = glue.get_crawler(Name=crawler_name)[\u0026#39;Crawler\u0026#39;][\u0026#39;State\u0026#39;] if crawler_state == \u0026#39;READY\u0026#39;: logger.info(f\u0026#34;✅ Crawler {crawler_name} finished.\u0026#34;) break time.sleep(10) job_name = \u0026#39;ProcessJob\u0026#39; try: while True: job_runs = glue.get_job_runs(JobName=job_name, MaxResults=1) if not job_runs[\u0026#39;JobRuns\u0026#39;]: break current_status = job_runs[\u0026#39;JobRuns\u0026#39;][0][\u0026#39;JobRunState\u0026#39;] if current_status in [\u0026#39;RUNNING\u0026#39;, \u0026#39;STARTING\u0026#39;, \u0026#39;STOPPING\u0026#39;]: logger.info(f\u0026#34;🕒 Glue Job {job_name} is currently {current_status}, waiting...\u0026#34;) time.sleep(15) else: break logger.info(f\u0026#34;🚀 Starting Glue Job: {job_name}\u0026#34;) response = glue.start_job_run(JobName=job_name) job_run_id = response[\u0026#39;JobRunId\u0026#39;] except Exception as e: logger.error(f\u0026#34;❌ Failed to start Glue Job: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#34;Error starting Glue job: {str(e)}\u0026#34; } logger.info(f\u0026#34;⏳ Waiting for Glue Job {job_name} to complete...\u0026#34;) while True: job_status = glue.get_job_run(JobName=job_name, RunId=job_run_id) state = job_status[\u0026#39;JobRun\u0026#39;][\u0026#39;JobRunState\u0026#39;] logger.info(f\u0026#34;🧪 Glue Job status: {state}\u0026#34;) if state in [\u0026#39;SUCCEEDED\u0026#39;, \u0026#39;FAILED\u0026#39;, \u0026#39;STOPPED\u0026#39;]: break time.sleep(15) if state != \u0026#39;SUCCEEDED\u0026#39;: logger.error(f\u0026#34;❌ Glue Job ended with status: {state}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#34;Glue Job failed or stopped with status: {state}\u0026#34; } logger.info(f\u0026#34;✅ Glue Job {job_name} completed successfully.\u0026#34;) second_crawler = \u0026#39;salesdatacrawler_processed\u0026#39; try: glue.start_crawler(Name=second_crawler) logger.info(f\u0026#34;✅ Successfully started crawler: {second_crawler}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#34;Workflow completed. Crawler {second_crawler} started.\u0026#34; } except Exception as e: logger.error(f\u0026#34;❌ Failed to start second crawler: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: f\u0026#34;Error starting second crawler {second_crawler}: {str(e)}\u0026#34; } Đừng quên log trạng thái để kiểm tra trong CloudWatch Logs.\n"
},
{
	"uri": "//localhost:1313/vi/4-glue-job/4.2-write-etl-script/",
	"title": "Viết script ETL",
	"tags": [],
	"description": "",
	"content": "4.2 Viết script ETL Trong script Python (Spark):\nĐọc dữ liệu từ bảng sales_raw Đổi tên cột nếu cần (order_id → id, v.v.) Ép kiểu dữ liệu phù hợp (ví dụ: date, float) Ghi ra s3://my-data-pipeline-bucket/processed/ dưới dạng Parquet hoặc CSV import sys from pyspark.context import SparkContext from awsglue.utils import getResolvedOptions from awsglue.context import GlueContext from awsglue.job import Job from pyspark.sql.functions import col, trim, regexp_replace, round from awsglue.dynamicframe import DynamicFrame args = getResolvedOptions(sys.argv, [\u0026#39;JOB_NAME\u0026#39;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[\u0026#39;JOB_NAME\u0026#39;], args) input_dyf = glueContext.create_dynamic_frame.from_catalog( database=\u0026#34;sales_analysis_db\u0026#34;, table_name=\u0026#34;raw_sales_dataset\u0026#34; ) df = input_dyf.toDF() for col_name in df.columns: if df.schema[col_name].dataType.simpleString() == \u0026#39;string\u0026#39;: df = df.withColumn( col_name, trim( regexp_replace( regexp_replace(col(col_name), \u0026#39;^\u0026#34;|\u0026#34;$\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;\\s+\u0026#39;, \u0026#39; \u0026#39; ) ) ) if \u0026#39;price\u0026#39; in df.columns and \u0026#39;quantity\u0026#39; in df.columns: df = df.withColumn(\u0026#34;revenue\u0026#34;, round(col(\u0026#34;price\u0026#34;) * col(\u0026#34;quantity\u0026#34;), 2)) output_dyf = DynamicFrame.fromDF(df, glueContext, \u0026#34;output_dyf\u0026#34;) output_path = \u0026#34;s3://your-data-pipeline-bucket/processed/\u0026#34; glueContext.write_dynamic_frame.from_options( frame=output_dyf, connection_type=\u0026#34;s3\u0026#34;, connection_options={ \u0026#34;path\u0026#34;: output_path, \u0026#34;partitionKeys\u0026#34;: [] }, format=\u0026#34;csv\u0026#34; ) glueContext.write_dynamic_frame.from_options( frame=output_dyf, connection_type=\u0026#34;s3\u0026#34;, connection_options={\u0026#34;path\u0026#34;: \u0026#34;s3://your-data-pipeline-bucket/processed/\u0026#34;}, format=\u0026#34;csv\u0026#34; ) job.commit() Nên dùng định dạng Parquet để tối ưu chi phí \u0026amp; hiệu năng khi query bằng Athena.\n"
},
{
	"uri": "//localhost:1313/vi/3-crawler-raw/3.3-run-crawler/",
	"title": "Chạy Crawler",
	"tags": [],
	"description": "",
	"content": "3.3 Chạy Glue Crawler Sau khi tạo xong, chọn crawler salesdatacrawler và nhấn Run Crawler. Kiểm tra bảng: Vào Glue Console → Databases → sales_db Đảm bảo bảng sales_raw đúng với dữ liệu từ sales.csv. Đây là bước xác minh schema được trích xuất chính xác.\n"
},
{
	"uri": "//localhost:1313/vi/4-glue-job/4.3-run-and-verify/",
	"title": "Chạy Job &amp; kiểm tra output",
	"tags": [],
	"description": "",
	"content": "4.3 Chạy Job \u0026amp; kiểm tra output Vào Glue → Jobs → chọn ProcessJob → Run Theo dõi trạng thái job: RUNNING → SUCCEEDED Mở S3 Bucket → thư mục processed/ Kiểm tra file được sinh ra có đúng định dạng: Parquet hoặc CSV "
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/7.3-attach-s3-trigger/",
	"title": "Gắn trigger S3 event",
	"tags": [],
	"description": "",
	"content": "7.3 Gắn trigger S3 event Truy cập S3 → chọn bucket chứa dữ liệu (my-data-pipeline-bucket) Vào tab Properties → Event notifications Thêm trigger: Prefix: raw/ Event type: PUT (ObjectCreated) Destination: Lambda function RunETLPipeline Mỗi khi có file mới trong thư mục raw/, Lambda sẽ tự động chạy pipeline.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduction/1.2-purpose-and-applications/",
	"title": "Mục đích và Ứng dụng",
	"tags": [],
	"description": "",
	"content": "Nội dung:\n🌐 Tại sao dùng AWS cho Data Pipeline? 📌 Trường hợp sử dụng thực tế 🚀 Khả năng mở rộng trong tương lai 🌐 Tại sao dùng AWS cho Data Pipeline? AWS cung cấp các dịch vụ mạnh mẽ như S3, Glue, và Athena, giúp xây dựng hệ thống xử lý dữ liệu hiệu quả, không cần quản lý hạ tầng phức tạp, và hỗ trợ tự động hóa toàn diện.\nVới mô hình tính phí theo mức sử dụng, khả năng mở rộng toàn cầu và tích hợp dịch vụ linh hoạt, AWS là lựa chọn hàng đầu cho các bài toán kỹ thuật dữ liệu hiện đại.\n📌 Trường hợp sử dụng thực tế 🔍 Làm sạch và chuyển đổi dữ liệu: Tự động xử lý, chuẩn hóa file CSV, JSON hoặc log với Glue 📊 Phân tích kinh doanh: Truy vấn trực tiếp dữ liệu trên S3 bằng SQL qua Athena 🏢 Chuẩn bị dữ liệu cho kho dữ liệu: Chuyển dữ liệu về dạng có cấu trúc để đưa vào Redshift hoặc công cụ BI khác 🔁 Cập nhật dữ liệu định kỳ: Chạy các job theo lịch để cập nhật dữ liệu báo cáo 🚀 Khả năng mở rộng trong tương lai Dễ dàng tích hợp thêm các dịch vụ như:\nAmazon QuickSight để trực quan hóa Amazon Redshift làm kho dữ liệu AWS Lambda để kích hoạt tự động theo sự kiện Hỗ trợ thêm nguồn dữ liệu và dung lượng lớn mà không cần thay đổi hệ thống hiện tại.\nKiến trúc có tính mô-đun, linh hoạt và “cloud-native”, phù hợp cho mọi nhu cầu mở rộng về sau.\n"
},
{
	"uri": "//localhost:1313/vi/3-crawler-raw/",
	"title": "Tạo Glue Crawler",
	"tags": [],
	"description": "",
	"content": "Tạo Glue Crawler Để AWS Glue hiểu được cấu trúc dữ liệu trong S3, bạn cần thực hiện 3 bước:\n3.1 Tạo Glue Database 3.2 Tạo Glue Crawler 3.3 Chạy Glue Crawler "
},
{
	"uri": "//localhost:1313/vi/2-prepare-environment/2.3-upload-sample-data/",
	"title": "Upload dữ liệu mẫu",
	"tags": [],
	"description": "",
	"content": "Upload dữ liệu mẫu Chuẩn bị tệp mẫu: sales.csv, định dạng CSV đơn giản (ví dụ: id, date, amount) Truy cập S3 Bucket đã tạo Upload tệp sales.csv vào thư mục raw/ "
},
{
	"uri": "//localhost:1313/vi/6-athena-analysis/6.3-advanced-queries/",
	"title": "Viết truy vấn nâng cao",
	"tags": [],
	"description": "",
	"content": "6.3 Viết các câu truy vấn nâng cao (tuỳ chọn) Một số ví dụ truy vấn phổ biến:\nTổng doanh thu theo sản phẩm: SELECT product, SUM(amount) AS total_amount FROM sales_db.sales_processed GROUP BY product; Doanh thu theo ngày: SELECT order_date, SUM(amount) AS total FROM sales_db.sales_processed GROUP BY order_date ORDER BY order_date; "
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/7.4-cloudtrail-monitoring/",
	"title": "Ghi lại hành vi người dùng với AWS CloudTrail",
	"tags": [],
	"description": "",
	"content": "Mục đích AWS CloudTrail giúp theo dõi và ghi nhật ký tất cả hoạt động của người dùng và dịch vụ AWS, bao gồm các thao tác trên S3, IAM, Lambda, và nhiều dịch vụ khác.\nCác bước thực hiện 1. Truy cập dịch vụ CloudTrail Vào AWS Console → Tìm CloudTrail → Nhấn Create trail 2. Tạo Trail mới Trail name: fcj-trail Storage location: Tạo hoặc chọn 1 S3 Bucket để chứa log Bật tùy chọn Log file validation Bật chế độ: Multi-region trail 3. Cấu hình nhật ký Chọn ghi nhật ký Management events Loại hành động: Write-only Bật tùy chọn: Data events → Chọn S3 → Add bucket: Bucket name: your-data-pipeline-bucket Event type: PUT (để ghi lại hành vi ghi dữ liệu) 4. Kiểm tra hoạt động Upload một file CSV mới vào S3 Truy cập lại CloudTrail → Event history Lọc theo EventName = PutObject và Resource = your-data-pipeline-bucket Bạn sẽ thấy ai (IAM user hoặc Lambda) đã thao tác, vào lúc nào Lợi ích Giúp kiểm tra xem ai đã upload file nào, lúc nào Phát hiện bất thường trong thao tác dữ liệu Phục vụ mục đích audit và bảo mật Gợi ý Bạn có thể kết hợp CloudTrail với:\nAWS CloudWatch Logs để giám sát theo thời gian thực AWS Config để theo dõi cấu hình tài nguyên "
},
{
	"uri": "//localhost:1313/vi/4-glue-job/",
	"title": "Tạo Glue Job",
	"tags": [],
	"description": "",
	"content": "Tạo Glue Job Để xử lý dữ liệu từ sales_raw và ghi kết quả ra thư mục processed/, bạn cần:\n4.1 Tạo Glue Job mới 4.2 Viết script ETL 4.3 Chạy Job \u0026amp; kiểm tra kết quả "
},
{
	"uri": "//localhost:1313/vi/5-crawler-processed/",
	"title": "Tạo Glue Crawler cho dữ liệu đã xử lý",
	"tags": [],
	"description": "",
	"content": "Tạo Glue Crawler cho dữ liệu đã xử lý Sau khi dữ liệu đã được xử lý và ghi vào thư mục processed/, chúng ta cần tạo một Glue Crawler mới để phát hiện schema của dữ liệu đầu ra.\nCác bước: 5.1 Tạo Crawler mới 5.2 Chạy Crawler và kiểm tra bảng "
},
{
	"uri": "//localhost:1313/vi/6-athena-analysis/",
	"title": "Truy vấn với Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Truy vấn với Amazon Athena Sau khi crawler đã tạo bảng sales_processed, bạn có thể sử dụng Amazon Athena để truy vấn dữ liệu trực tiếp trên S3.\nCác bước: 6.1 Cấu hình Athena 6.2 Truy vấn dữ liệu đã xử lý 6.3 Viết các câu truy vấn nâng cao (tuỳ chọn) "
},
{
	"uri": "//localhost:1313/vi/7-automate-with-lambda/",
	"title": "Tự động hóa bằng AWS Lambda",
	"tags": [],
	"description": "",
	"content": "Tự động hóa bằng AWS Lambda Để tự động hóa toàn bộ pipeline, chúng ta sử dụng AWS Lambda để kích hoạt quá trình ETL khi có file mới.\nCác bước: 7.1 Tạo Lambda Function 7.2 Viết mã Python cho Lambda 7.3 Gắn trigger S3 event "
},
{
	"uri": "//localhost:1313/vi/8-quicksight-dashboard/",
	"title": "Trực quan hóa với Amazon QuickSight",
	"tags": [],
	"description": "",
	"content": "Trực quan hóa với Amazon QuickSight (Tuỳ chọn) Sau khi dữ liệu đã được xử lý và lưu trong sales_processed, bạn có thể sử dụng Amazon QuickSight để tạo các dashboard trực quan.\nCác bước: 8.1 Kết nối Athena với QuickSight 8.2 Tạo dashboard trực quan "
},
{
	"uri": "//localhost:1313/vi/9-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Dọn dẹp tài nguyên Sau khi hoàn tất pipeline, bạn nên dọn dẹp các tài nguyên AWS để tránh phát sinh chi phí không cần thiết.\n🧹 Xoá tài nguyên theo danh mục: AWS Glue Xoá Crawler: salesdatacrawler, salesdatacrawler_processed Xoá Job: ProcessJob Xoá Database: sales_db AWS Lambda Xoá function: RunETLPipeline Amazon S3 Xoá bucket: my-data-pipeline-bucket (hoặc chỉ xoá thư mục raw/, processed/ nếu giữ bucket) IAM Xoá các IAM Role nếu không tái sử dụng: AWSGlueServiceRole AWSLambdaExecutionRole CloudWatch Vào CloudWatch → Logs → Xoá log group của Lambda QuickSight Vào QuickSight: Xoá dataset sales_processed Xoá dashboard nếu đã tạo "
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]